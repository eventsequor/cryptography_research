Due to their protagonic relevance in cryptography and security, as mentioned before, the evaluation of random and pseudorandom number generators (RNGs/PRNGs) is a critical aspect of their deployment. The statistical properties of the generated sequences must approximate those of true random sequences to ensure unpredictability and reliability. The National Institute of Standards and Technology (NIST) Special Publication 800-22 Rev 1a provides a widely recognized suite of statistical tests for this purpose, offering tools to detect deviations from randomness in binary sequences. While no finite set of tests can provide absolute certification of randomness, these tests serve as an essential initial step in assessing a generator's suitability.

The five tests selected from the NIST suite for this analysis---Frequency (Monobit) Test, Frequency Test within a Block, Runs Test, Maurer's "Universal Statistical" Test, and the Cumulative Sums (Cusum) Test---were chosen to provide a balanced and comprehensive assessment. This selection emphasizes tests that are both fundamental in their approach and offer insights into different potential weaknesses of an RNG, ranging from basic distributional properties and sequential dependencies to more complex characteristics like data compressibility, which relates to the sequence's entropy. The aim is to cover a diverse range of statistical attributes with tests that are also relatively accessible in their conceptual underpinnings, facilitating a clear interpretation of a generator's performance.

\smallskip

\textbf{Selected Statistical Tests: Description and Interpretation}

\smallskip

\subsection{\textbf{Frequency (Monobit) Test}}

The \textbf{Frequency (Monobit) Test} is the most fundamental test, examining the overall proportion of zeros and ones in the entire binary sequence. Its purpose is to determine if the number of ones and zeros in a sequence are approximately equal, as would be expected for a truly random sequence. The test assesses the closeness of the fraction of ones to $1/2$ by converting the bits into a sequence of $+1$ and $-1$ values, summing these values to obtain $S_n$, and then computing a test statistic $s_{\text{obs}} = |S_n| / \sqrt{n}$. A P-value is subsequently calculated using the complementary error function, $P\text{-value} = \text{erfc}(s_{\text{obs}} / \sqrt{2})$. If the computed P-value is less than the chosen significance level $\alpha$ (typically $0.01$), the sequence is considered non-random; otherwise, it is considered random with respect to this test. A small P-value suggests that the sum $S_n$ is too large, indicating an excess of ones or zeros in the sequence. NIST recommends a minimum sequence length of 100 bits for this test.

\subsection{\textbf{Frequency Test within a Block}}

The \textbf{Frequency Test within a Block} focuses on the proportion of ones within M-bit blocks of the sequence. Its objective is to determine whether the frequency of ones in an M-bit block is approximately $M/2$, as would be expected under an assumption of randomness. The input sequence is partitioned into $N = \lfloor n/M \rfloor$ non-overlapping blocks. For each block, the proportion of ones ($\pi_i$) is calculated, and a $\chi^2$ statistic is computed as $\chi^2(\text{obs}) = 4M \sum_{i=1}^{N}(\pi_i - 1/2)^2$. The P-value is then determined using the incomplete gamma function ($\text{igamc}(N/2, \chi^2(\text{obs})/2)$). If the P-value is less than $\alpha$ (e.g., $0.01$), it suggests non-randomness, indicating that at least one block deviates significantly from the expected equal proportion of ones and zeros. Recommended input parameters include $n \ge 100$, $M \ge 20$, $M > 0.01n$, and $N < 100$.



\subsection{\textbf{Runs Test}}

The \textbf{Runs Test} examines the total number of runs in the sequence, where a run is defined as an uninterrupted sequence of identical bits. The purpose is to determine if the oscillation between zeros and ones is too fast or too slow, which would indicate a deviation from randomness. A prerequisite is that the sequence passes a basic frequency check: if the proportion of ones, $\pi$, satisfies $|\pi - 1/2| \ge \tau$, where $\tau = 2/\sqrt{n}$, the Runs Test is not performed. If applicable, the test statistic $V_n(\text{obs})$, the total observed number of runs, is calculated. The P-value is then $P\text{-value} = \text{erfc}(|V_n(\text{obs}) - 2n\pi(1-\pi)| / (2\sqrt{2n}\pi(1-\pi)))$. A P-value less than $\alpha$ (e.g., $0.01$) leads to the conclusion that the sequence is non-random. A large value of $V_n(\text{obs})$ suggests the oscillation is too fast, while a small value indicates it is too slow. A minimum sequence length of 100 bits is recommended.



\subsection{\textbf{Maurer's "Universal Statistical" Test}}

\textbf{Maurer's "Universal Statistical" Test} evaluates the compressibility of the sequence, focusing on the number of bits between matching L-bit patterns. The test aims to detect whether a sequence can be significantly compressed without loss of information; a significantly compressible sequence is considered non-random. The $n$-bit sequence is divided into an initialization segment of Q L-bit blocks and a test segment of K L-bit blocks. For each L-bit block in the test segment, the algorithm identifies the distance (in blocks) to its most recent previous occurrence found in the sequence processed so far. The test statistic, $f_n$, is the average of the base-2 logarithm of these distances over all K test blocks. This observed $f_n$ is compared to a theoretical expected value for the given L, $\text{expectedValue}(L)$, and a P-value is computed as $P\text{-value} = \text{erfc}(|f_n - \text{E}(L)| / (\sqrt{2}\sigma))$, where $\sigma$ is a pre-calculated standard deviation. If the P-value is less than $\alpha$ (e.g., $0.01$), the sequence is deemed non-random, suggesting it is compressible. This test requires long sequences, with L typically between 6 and 16, and $n$ chosen such that $K \approx 1000 \cdot 2^L$ and $Q = 10 \cdot 2^L$.

\subsection{\textbf{Cumulative Sums (Cusum) Test}}

The \textbf{Cumulative Sums (Cusum) Test} focuses on the maximal excursion (from zero) of a random walk defined by the cumulative sum of adjusted (-1, +1) digits in the sequence. The purpose is to determine if this cumulative sum is too large or too small relative to the expected behavior for random sequences. The (0,1) sequence is converted to a sequence of $X_i$ values of -1 and +1. Partial sums $S_k$ are computed. The test statistic $z$ is the maximum absolute value among these partial sums ($z = \max_{1 \le k \le n} |S_k|$). The test can be applied in a forward direction (mode=0) or a backward direction (mode=1). The P-value is calculated based on $z$ and $n$, involving sums of normal cumulative distribution function evaluations. If the P-value is less than $\alpha$ (e.g., $0.01$), the sequence is considered non-random. Large excursions suggest an excess of ones or zeros at the beginning (forward mode) or end (backward mode) of the sequence, while very small excursions might indicate that ones and zeros are intermixed too evenly. A minimum sequence length of 100 bits is recommended.

Additionally, we presented a visual test assesses byte sequence randomness by plotting a histogram of byte value frequencies (0-255). Ideally random data yields a flat, uniform histogram, as each byte value should occur with similar probability. Overlays like a normal distribution (using sample $\mu$ and $\sigma$) and highlighting extreme frequencies help visualize this. Significant deviations from uniformity, such as pronounced peaks or valleys, suggest potential non-randomness in the sequence.